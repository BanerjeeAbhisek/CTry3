---
title: "Normalizing Flows"
author: "Abhisek Banerjee"
from: markdown+raw_html
execute:
  echo: false

filters:
  - quarto

# IMPORTANT:    
html-math-method: katex

format:
  revealjs:
    theme: [default, custom.scss]
    css: styles.css

    # Slide geometry
    width: 1280
    height: 720
    margin: 0.05
    center: false

    # Navigation & behavior
    transition: slide
    background-transition: fade
    slide-number: true
    progress: true
    controls: true
    hash: true
    preview-links: true
    incremental: false
    chalkboard: true
    code-line-numbers: true

    # Math rendering
    html-math-method: katex

    # Tiger-stripe background ONLY for title slide
    title-slide-attributes:
      data-background-image: "assets/tigerstripes.png"
      data-background-size: "cover"
      data-background-position: "center"

    # Fonts & head includes
    include-in-header:
      text: |
        <script src="https://cdn.jsdelivr.net/npm/d3@7"></script>
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;800;900&display=swap" rel="stylesheet">

    # Footer + Home button
    include-after-body:
      text: |
        <div class="mizzou-footer-bar">
          <div class="mizzou-left">
            <img src="assets/mizzou-logo.svg" alt="Mizzou logo">
          </div>
          <div class="mizzou-right">
            Department of Statistics / Class 9100
          </div>
        </div>

        <a class="home-btn" href="#outline" title="Go to Outline" aria-label="Go to Outline">
          <svg viewBox="0 0 24 24" aria-hidden="true">
            <path d="M3 10.5L12 3l9 7.5V21a1 1 0 0 1-1 1h-5v-6H9v6H4a1 1 0 0 1-1-1z"/>
          </svg>
        </a>
        - flow-demo.js

resources:
  - styles.css
  - assets
---


## Outline {#outline}


<div class="toc-card">

1. [Motivation](#motivation)
2. [Normalizing Flows](#NormFlows)  
3. [Inference with Normalizing Flows](#InfNormFlow)  
4. [Algorithm of Normalizing Flows](#AlgoNormFlows) 
5. [Bayes Flow](#BFlowStart)  
6. [Algorithm of Bayes Flow](#AlgoBayesFlow)
7. [Recent Developments in Normalizing Flows](#DevelopNormFlow)
8. [Summary of Normalizing Flow Families](#SummNormFlow)
9. [Future Directions of Normalizing Flows](#FutureDir)

</div>






## Motivation  {#motivation}

### Variational inference: the core problem

We consider a latent variable model with observations $x$ and latent variables $z$ and parameter $\theta$.

The posterior
$$
p_\theta(z \mid x)
$$
is typically **intractable**.


### Variational idea  

Introduce a tractable approximation
$$
q_\phi(z \mid x) \approx p_\theta(z \mid x),
$$
and optimize the evidence lower bound (ELBO):
$$
\log p_\theta(x)
\;\ge\;
\mathbb{E}_{q_\phi(z \mid x)}\!\left[\log p_\theta(x \mid z)\right]
-
\mathrm{D_{KL}}\!\left(q_\phi(z \mid x)\,\|\,p(z)\right).
$$



---

## Reparameterization trick

Assume a Gaussian variational posterior:
$$
q_\phi(z \mid x) = \mathcal{N}\!\big(z | \mu(x), \sigma^2(x)\big).
$$

<br>

### Reparameterize

Instead of sampling $z \sim q_\phi$, write:
$$
z = \mu(x) + \sigma(x)\,\varepsilon,
\qquad
\varepsilon \sim \mathcal{N}(0, I).
$$

<br>

### Result

Gradients move **inside** the expectation:
$$
\nabla_\phi
\mathbb{E}_{q_\phi(z \mid x)}[f(z)]
=
\mathbb{E}\!\left[
\nabla_\phi f\big(\mu(x)+\sigma(x)\varepsilon\big)
\right].
$$
---

## The problem with standard amortized VI

In practice, $q_\phi(z \mid x)$ is chosen to be **simple**:

$$
q_\phi(z \mid x)
=
\mathcal{N}\!\big(z | \mu_\phi(x), \mathrm{diag}(\sigma_\phi^2(x))\big).
$$



### Consequences

- Mean-field independence assumptions
- Cannot represent:
  - multimodality
  - strong correlations
  - complex posterior geometry



### Empirical issues (well documented)

- Underestimation of posterior variance
- Biased parameter estimates
- Poor uncertainty quantification


---

## What do we want from a variational posterior?

An ideal variational family should be:

- Flexible enough to approximate the true posterior
- Compatible with reparameterization
- Scalable with amortized inference
- Computationally tractable

<br>

### Key question

Can we start with a simple distribution
and **systematically increase its complexity**?

<br>

```{ojs}


flow_strip_anim = {
  const W = 920, H = 170, pad = 10;
  const panels = 4, panelW = W / panels;

  // sample base points
  const n = 320;
  const pts0 = d3.range(n).map(() => {
    const u1 = Math.random(), u2 = Math.random();
    const r = Math.sqrt(-2*Math.log(u1));
    const a = 2*Math.PI*u2;
    return [r*Math.cos(a), r*Math.sin(a)];
  });

  // STRONGER, visually distinct warps
  const f1 = ([x,y]) => [x + 1.2*Math.tanh(1.2*y), y];                   // big shear
  const f2 = ([x,y]) => {
    const th = 0.55*Math.tanh(0.9*x);                                    // local rotation
    const c = Math.cos(th), s = Math.sin(th);
    return [c*x - s*y, s*x + c*y];
  };
  const f3 = ([x,y]) => {
    const r = Math.sqrt(x*x + y*y) + 1e-6;
    const k = 0.9*Math.tanh(1.0*(r-1.1));                                // twist/radial
    return [x + k*(-y), y + k*(x)];
  };

  const steps = [
    {name: "zâ‚€", pts: pts0},
    {name: "zâ‚", pts: pts0.map(f1)},
    {name: "zâ‚‚", pts: pts0.map(f1).map(f2)},
    {name: "zâ‚ƒ", pts: pts0.map(f1).map(f2).map(f3)}
  ];

  const xS = d3.scaleLinear().domain([-4,4]).range([pad, panelW-pad]);
  const yS = d3.scaleLinear().domain([-4,4]).range([H-pad, pad]);

  const svg = d3.create("svg")
    .attr("viewBox", `0 0 ${W} ${H}`)
    .style("width","100%")
    .style("max-width","920px")
    .style("height","170px")
    .style("border","1px solid rgba(255,255,255,0.18)")
    .style("border-radius","10px");

  // panel frames + labels
  steps.forEach((s, i) => {
    const g = svg.append("g").attr("transform", `translate(${i*panelW},0)`);

    g.append("rect")
      .attr("x", 2).attr("y", 2)
      .attr("width", panelW-4).attr("height", H-4)
      .attr("rx", 10)
      .attr("fill", "transparent")
      .attr("stroke", "rgba(255,255,255,0.12)");

    g.append("text")
      .attr("x", 10).attr("y", 18)
      .attr("font-size", 14)
      .attr("font-weight", 650)
      .text(s.name);

    if (i < panels-1) {
      svg.append("text")
        .attr("x", (i+1)*panelW - 10)
        .attr("y", H/2 + 6)
        .attr("font-size", 22)
        .attr("opacity", 0.55)
        .text("â†’");
    }
  });

  // draw dots in each panel (same indices, so we can interpolate)
  const dotGroups = steps.map((s, i) => {
    return svg.append("g")
      .attr("transform", `translate(${i*panelW},0)`)
      .selectAll("circle")
      .data(pts0.map((p, idx) => ({idx, p0: p})))
      .join("circle")
        .attr("r", 1.35)
        .attr("opacity", 0.75);
  });

  // helper: interpolate between step a and b
  function lerp(a,b,t){ return a + (b-a)*t; }

  function render(tms){
    // stage cycles: (0â†’1) for z0->z1, then z1->z2, then z2->z3
    const cycle = (tms/1200) % 3;             // 0..3
    const k = Math.floor(cycle);              // 0,1,2
    const t = cycle - k;                      // 0..1

    for (let i=0; i<steps.length; i++){
      // Each panel shows its "target" step, but animates the transition for the current stage
      let from = i, to = i;

      if (k === 0 && i === 1) { from = 0; to = 1; }   // animate z1
      if (k === 1 && i === 2) { from = 1; to = 2; }   // animate z2
      if (k === 2 && i === 3) { from = 2; to = 3; }   // animate z3

      const A = steps[from].pts;
      const B = steps[to].pts;

      dotGroups[i]
        .attr("cx", d => {
          const a = A[d.idx], b = B[d.idx];
          return xS(lerp(a[0], b[0], t));
        })
        .attr("cy", d => {
          const a = A[d.idx], b = B[d.idx];
          return yS(lerp(a[1], b[1], t));
        });
    }

    requestAnimationFrame(render);
  }

  requestAnimationFrame(render);
  return svg.node();
}
``` 


## Normalizing flows  {#NormFlows}


<br>

<div class="mizzou-gold-box">

A **normalizing flow** constructs a complex density by transforming
a simple one through invertible mappings.

By repeatedly applying the rule for change of variables, the initial density â€˜flowsâ€™ through the sequence of invertible mappings.

</div>

<br>

### Construction

Start with
$$
z_0 \sim q_0(z_0 \mid x),
$$
and apply a sequence of invertible transformations:
$$
z_k = f_k(z_{k-1}), \qquad k=1,\dots,K.
$$

The final variable $z_K$ has density $q_K(z_K \mid x)$.

---

## Change of variables

By the change-of-variables formula:
$$
\log q_K(z_K \mid x)
=
\log q_0(z_0 \mid x)
-
\sum_{k=1}^K
\log\left|
\det
\frac{\partial f_k}{\partial z_{k-1}}
\right|.
$$



### Interpretation

- Each transformation **warps** the density
- Stacking transformations yields rich posteriors
- Complexity controlled by flow length $K$

### Why normalizing flows help

- Start from a simple, reparameterizable base distribution
- Add flexibility through invertible transformations
- Preserve tractable likelihoods and gradients

---

## Finite Normalizing flows


Let
$$
f:\mathbb{R}^d \to \mathbb{R}^d
$$
be a smooth, invertible map with inverse $f^{-1}$.

If
$$
z \sim q(z),
\qquad
z' = f(z),
$$
then $z'$ has density
$$
q(z') = q(z)\left|\det\frac{\partial f^{-1}}{\partial z'}\right|
      = q(z)\left|\det\frac{\partial f}{\partial z}\right|^{-1}.
$$
---

## Finite normalizing flows

Rather than a single transformation, we compose many:

$$
z_K = f_K \circ f_{K-1} \circ \cdots \circ f_1(z_0),
$$

where
$$
z_0 \sim q_0(z_0).
$$


The resulting density is
$$
\log q_K(z_K)
=
\log q_0(z_0)
-
\sum_{k=1}^K
\log\left|
\det\frac{\partial f_k}{\partial z_{k-1}}
\right|.
$$

---

## Why this is called a *flow*

- The sequence
  $$
  z_0 \rightarrow z_1 \rightarrow \cdots \rightarrow z_K
  $$
  traces a **path in latent space**

- The corresponding sequence of densities
  $$
  q_0 \rightarrow q_1 \rightarrow \cdots \rightarrow q_K
  $$
  forms a **normalizing flow**

<br>

With increasing $K$, the density can become:
- non-Gaussian
- correlated
- multi-modal

```{ojs}
flow_path_anim = {
  const W = 920, H = 230, pad = 16;
  const K = 5;                 // show z0..z5
  const panelW = (W - 2*pad) / (K+1);

  // sample base points (standard normal)
  const n = 260;
  const pts0 = d3.range(n).map(() => {
    const u1 = Math.random(), u2 = Math.random();
    const r = Math.sqrt(-2*Math.log(u1));
    const a = 2*Math.PI*u2;
    return [r*Math.cos(a), r*Math.sin(a)];
  });

  // simple "flow-like" warps (deterministic, invertible-looking)
  const f1 = ([x,y]) => [x + 0.95*Math.tanh(1.1*y), y];                     // shear
  const f2 = ([x,y]) => {
    const th = 0.55*Math.tanh(0.8*x);                                      // local rotation
    const c = Math.cos(th), s = Math.sin(th);
    return [c*x - s*y, s*x + c*y];
  };
  const f3 = ([x,y]) => {
    const r = Math.sqrt(x*x + y*y) + 1e-6;
    const k = 0.75*Math.tanh(0.9*(r-1.1));                                 // radial twist
    return [x + k*(-y), y + k*(x)];
  };

  const Fs = [p => p, f1, p => f2(f1(p)), p => f3(f2(f1(p))), p => f1(f3(f2(f1(p)))), p => f2(f1(f3(f2(f1(p)))))];

  // precompute pts at each k (z_k)
  const steps = d3.range(K+1).map(k => pts0.map(Fs[k]));

  const xS = d3.scaleLinear().domain([-4,4]).range([pad, panelW - pad]);
  const yS = d3.scaleLinear().domain([-4,4]).range([H - pad, pad]);

  const svg = d3.create("svg")
    .attr("viewBox", `0 0 ${W} ${H}`)
    .style("width","100%")
    .style("max-width","920px")
    .style("height","230px")
    .style("border","1px solid rgba(255,255,255,0.18)")
    .style("border-radius","12px");

  // panel frames + labels
  for (let i=0; i<=K; i++){
    const x0 = pad + i*panelW;

    svg.append("rect")
      .attr("x", x0 + 4).attr("y", 8)
      .attr("width", panelW - 8).attr("height", H - 16)
      .attr("rx", 12)
      .attr("fill","transparent")
      .attr("stroke","rgba(255,255,255,0.12)");

    svg.append("text")
      .attr("x", x0 + 12).attr("y", 26)
      .attr("font-size", 13.5)
      .attr("font-weight", 750)
      .attr("opacity", 0.92)
      .text(`z_${i}`);

    if (i < K){
      svg.append("text")
        .attr("x", x0 + panelW - 6)
        .attr("y", H/2 + 6)
        .attr("font-size", 22)
        .attr("opacity", 0.55)
        .text("â†’");
    }
  }

  // dots per panel (same indices, so we can interpolate)
  const dotGroups = d3.range(K+1).map(i => {
    const gx = svg.append("g").attr("transform", `translate(${pad + i*panelW},0)`);
    return gx.selectAll("circle")
      .data(pts0.map((p, idx) => ({idx})))
      .join("circle")
        .attr("r", 1.55)
        .attr("opacity", 0.78);
  });

  const lerp = (a,b,t) => a + (b-a)*t;

  function render(tms){
    // stage cycles through transitions: z0->z1->...->zK
    const speed = 1100;
    const cycle = (tms / speed) % K;        // 0..K
    const k = Math.floor(cycle);            // transition k -> k+1
    const t = cycle - k;                    // 0..1

    for (let i=0; i<=K; i++){
      // each panel shows its final step, but panel i animates when it is the "current" step
      let from = i, to = i;
      if (i === k+1) { from = k; to = k+1; }   // animate the next panel filling in

      const A = steps[from];
      const B = steps[to];

      dotGroups[i]
        .attr("cx", d => {
          const a = A[d.idx], b = B[d.idx];
          return xS(lerp(a[0], b[0], t));
        })
        .attr("cy", d => {
          const a = A[d.idx], b = B[d.idx];
          return yS(lerp(a[1], b[1], t));
        });
    }

    requestAnimationFrame(render);
  }

  requestAnimationFrame(render);
  return svg.node();
}


```

---

## Expectations under a flow (LOTUS)

A key property:

For any function $h(z)$,
$$
\mathbb{E}_{q_K}[h(z)]
=
\mathbb{E}_{q_0}\!\left[
h\big(f_K \circ \cdots \circ f_1(z_0)\big)
\right].
$$

<br>

### Consequence

- We can compute expectations **without explicitly evaluating** $q_K$
- Jacobian terms only matter when evaluating $\log q_K$

---

## Geometric intuition

Effect of invertible flows can be thought of as a sequence of expansions or contractions on the initial density

- **Expansions**  
  â†’ The map $z'=f(z)$ pulls the pointz $z$ away from a region in $\mathbb{R}^d$, reducing density in that region.

- **Contractions**  
  â†’ concentrate mass, increase density locally


By composing expansions and contractions,
a simple base density can be warped into a highly complex one.

```{ojs}
d3 = require("d3@7")

flow_viz = {
  // â†“â†“â†“ reduced size â†“â†“â†“
  const W = 520, H = 260, pad = 18;

  const xs = d3.scaleLinear().domain([-2,2]).range([pad, W-pad]);
  const ys = d3.scaleLinear().domain([-1.4,1.4]).range([H-pad, pad]);

  // â†“â†“â†“ slightly sparser grid â†“â†“â†“
  const pts = [];
  for (let x=-2; x<=2.001; x+=0.22)
    for (let y=-1.4; y<=1.4001; y+=0.22)
      pts.push([x,y]);

  const svg = d3.create("svg")
    .attr("viewBox", `0 0 ${W} ${H}`)
    .style("width","100%")
    .style("max-width","520px")
    .style("height","260px")
    .style("border","1px solid rgba(255,255,255,0.18)")
    .style("border-radius","10px");

  const label = svg.append("text")
    .attr("x", 10).attr("y", 18)
    .attr("font-size", 14)
    .attr("font-weight", 600);

  const dots = svg.selectAll("circle")
    .data(pts)
    .join("circle")
      .attr("r", 1.6)
      .attr("opacity", 0.75);

  function frame(tms){
    const t = (Math.sin(tms/900) + 1) / 2;
    const strength = (2*t - 1) * 0.55;

    label.text(strength >= 0
      ? "Expansion"
      : "Contraction"
    );

    const warped = pts.map(([x0,y0]) => {
      const r2 = x0*x0 + y0*y0 + 0.15;
      const s = strength / r2;
      return [x0 + s*x0, y0 + s*y0];
    });

    dots
      .data(warped)
      .attr("cx", d => xs(d[0]))
      .attr("cy", d => ys(d[1]));

    requestAnimationFrame(frame);
  }

  requestAnimationFrame(frame);
  return svg.node();
}

```


## Infinitestimal Flows

We define the variational posterior as
$$
q_\phi(z \mid x) := q_K(z_K),
$$

where the transformations $f_k$ are parameterized by $\phi$


### Key idea

Complexity is controlled by **flow length** $K$,


What happens as the flow length $K \to \infty$?


Instead of discrete transformations,
the density evolves continuously in time:
$$
\frac{\partial}{\partial t} q_t(z) = \mathcal{T}_t[q_t(z)],
$$

where $\mathcal{T}_t$ defines continuous-time dynamics.


---


::: columns
::: column
### Langevin flow

One important infinitesimal flow is the Langevin SDE:
$$
dz(t) = F(z(t),t)\,dt + G(z(t),t)\,d\xi(t),
$$

where $d\xi(t)$ is a Wiener process, $F$ is a drift vector, $D=GG'$ is a diffusion matrix. 


<br>
In machine learning, commonly:
$$
F(z,t) = -\nabla_z \mathcal{L}(z),
\qquad
G(z,t) = \sqrt{2}\,I.
$$
where $\mathcal{L}(z)$ is a unnormalised log-density of the model.
:::



::: column
### Hamiltonian flows

Hamiltonian Monte Carlo can also be viewed as a normalizing flow
on an augmented space $(z,\omega)$.

<br>

Its dynamics results from Hamiltonian $$\mathcal{H}(z,w)=-\mathcal{L}(z)-\frac{1}{2}\omega'M\omega$$

<br>

Also widely used in machine learning.

:::
:::

---

## Inference with normalizing flows {#InfNormFlow}

### Invertible linear-time transformations
<br>

To construct expressive variational posteriors, we need transformations that are:

- **Invertible** (to define a valid density)
- **Differentiable** (for gradients)
- **Computationally efficient** (Jacobian determinant is cheap)

<br>



<br>

<div class="mizzou-gold-box">
The key idea is to design flows whose Jacobian determinants can be computed in **linear time** $O(d)$.
</div>

---

## Invertible Linear-time Transformations


Consider transformations of the form:
$$
f(z) = z + u\, h(w^\top z + b),
$$

where $u, w \in \mathbb{R}^d$, $b \in \mathbb{R}$ $\&$ $h(\cdot)$ is a smooth scalar nonlinearity

### Jacobian determinant (planar flow)

Define
$$
\psi(z) = h'(w^\top z + b)\, w.
$$

Then the Jacobian determinant simplifies to:
$$
\left| \det \frac{\partial f}{\partial z} \right|
=
\left| 1 + u^\top \psi(z) \right|.
$$


### Consequences

Determinant computed in **\(O(d)\) time**

Density is tractable:
$\log q_K(z_K)
=
\log q_0(z_0)
-
\sum_{k=1}^K
\log | 1 + u_k^\top \psi_k(z_k) |$

---

## Flow-based free energy bound

We parameterize the approximate posterior using a flow of length \(K\):
$$
q_\phi(z \mid x) := q_K(z_K),
\qquad
z_K = f_K \circ \cdots \circ f_1(z_0),
\quad
z_0 \sim q_0(z_0).
$$

<br>

### Free energy (ELBO)

The variational free energy can be written as an expectation
over the **base distribution** $q_0$:
$$
\mathcal{F}(x)
=
\mathbb{E}_{q_0(z_0)}\!\left[
\log q_0(z_0)
-
\log p(x, z_K)
-
\sum_{k=1}^K
\log \left| 1 + u_k^\top \psi_k(z_k) \right|
\right].
$$

<br>

### Interpretation

- Optimization is performed **in the base space** $z_0$
- Normalizing flows and this free energy bound can be used with any variational optimization scheme, including generalized variational EM.

---

## Algorithm summary (normalizing flows) {#AlgoNormFlows}

Normalizing flows extend **amortized variational inference**
by augmenting the variational posterior with a sequence of
invertible transformations.


```{ojs}

algo_diagram = {
  const W = 980, H = 260;

  const svg = d3.create("svg")
    .attr("viewBox", `0 0 ${W} ${H}`)
    .style("width","100%")
    .style("max-width","980px")
    .style("height","260px");

  // ---- layout ----
  const nodes = [
    {id:"mb",  x:60,  y:60,  w:170, h:54, title:"Mini-batch",   sub:"x â† {get mini-batch}"},
    {id:"q0",  x:270, y:60,  w:200, h:54, title:"Base sample",  sub:"zâ‚€ ~ qâ‚€(zâ‚€ | x)"},
    {id:"flow",x:520, y:60,  w:240, h:54, title:"Flow map",     sub:"z_K = f_Kâˆ˜â€¦âˆ˜fâ‚(zâ‚€)"},
    {id:"F",   x:810, y:60,  w:150, h:54, title:"Free energy",  sub:"ð“•(x) â‰ˆ ð“•(x,z_K)"},

    {id:"gth", x:620, y:170, w:210, h:54, title:"Update Î¸",     sub:"Î”Î¸ âˆ âˆ’âˆ‡_Î¸ ð“•(x)"},
    {id:"gph", x:360, y:170, w:210, h:54, title:"Update Ï•",     sub:"Î”Ï• âˆ âˆ’âˆ‡_Ï• ð“•(x)"}
  ];

  const edges = [
    ["mb","q0"], ["q0","flow"], ["flow","F"],
    ["F","gth"], ["F","gph"]
  ];

  // ---- defs: arrow ----
  svg.append("defs").append("marker")
    .attr("id","arrow")
    .attr("viewBox","0 0 10 10")
    .attr("refX", 9).attr("refY", 5)
    .attr("markerWidth", 8).attr("markerHeight", 8)
    .attr("orient", "auto-start-reverse")
    .append("path")
    .attr("d","M 0 0 L 10 5 L 0 10 z")
    .attr("fill","rgba(0,0,0,0.45)");

  // helper
  const byId = new Map(nodes.map(d => [d.id, d]));
  const centerRight = d => [d.x + d.w, d.y + d.h/2];
  const centerLeft  = d => [d.x,       d.y + d.h/2];
  const centerTop   = d => [d.x + d.w/2, d.y];
  const centerBot   = d => [d.x + d.w/2, d.y + d.h];

  // ---- edges ----
  svg.append("g")
    .selectAll("path")
    .data(edges)
    .join("path")
      .attr("fill","none")
      .attr("stroke","rgba(0,0,0,0.35)")
      .attr("stroke-width", 2)
      .attr("marker-end","url(#arrow)")
      .attr("d", ([a,b]) => {
        const A = byId.get(a), B = byId.get(b);

        // horizontal main chain
        if (["mb","q0","flow"].includes(a) && ["q0","flow","F"].includes(b)) {
          const [x1,y1] = centerRight(A);
          const [x2,y2] = centerLeft(B);
          const mx = (x1+x2)/2;
          return `M${x1},${y1} C${mx},${y1} ${mx},${y2} ${x2},${y2}`;
        }

        // down from F to updates
        if (a === "F") {
          const [x1,y1] = centerBot(A);
          const [x2,y2] = centerTop(B);
          const mx = (x1+x2)/2;
          return `M${x1},${y1} C${x1},${y1+30} ${x2},${y2-30} ${x2},${y2}`;
        }
        return "";
      });

  // ---- nodes ----
  const g = svg.append("g").selectAll("g.node")
    .data(nodes)
    .join("g")
      .attr("class","node")
      .attr("transform", d => `translate(${d.x},${d.y})`);

  g.append("rect")
    .attr("width", d => d.w)
    .attr("height", d => d.h)
    .attr("rx", 14)
    .attr("fill", "rgba(255,255,255,0.85)")
    .attr("stroke", "rgba(0,0,0,0.18)")
    .attr("stroke-width", 1.5);

  g.append("text")
    .attr("x", 14).attr("y", 22)
    .attr("font-size", 14)
    .attr("font-weight", 800)
    .text(d => d.title);

  g.append("text")
    .attr("x", 14).attr("y", 42)
    .attr("font-size", 13)
    .attr("opacity", 0.85)
    .text(d => d.sub);

  // ---- cycling highlight ----
  const hi = svg.append("rect")
    .attr("rx", 14)
    .attr("fill", "rgba(234,170,0,0.10)")
    .attr("stroke", "rgba(234,170,0,0.75)")
    .attr("stroke-width", 3)
    .attr("pointer-events","none");

  const cycle = ["mb","q0","flow","F","gth","gph"]; // matches pseudo-code order + both updates
  let idx = 0;

  function step(){
    const d = byId.get(cycle[idx]);
    hi.attr("x", d.x).attr("y", d.y).attr("width", d.w).attr("height", d.h);
    idx = (idx + 1) % cycle.length;
    setTimeout(step, 900);
  }
  step();

  // small label: while loop
  svg.append("text")
    .attr("x", 60)
    .attr("y", 28)
    .attr("font-size", 14)
    .attr("font-weight", 700)
    .attr("opacity", 0.8)
    .text("Repeat until converged");

  return svg.node();
}

``` 


### Computational complexity

A key advantage of normalizing flows is that
they add **minimal computational overhead**.

- Jacobian determinant computed in **$O(D)$**
- No matrix inverses required
- Total cost scales **linearly in flow length** $K$
- Compatible with minibatch SGD and backpropagation

---

##  Bayes Flows{#BFlowStart}

### Why BayesFlow?

Many scientific models are **simulator-based**:

- We can simulate data $x \sim p(x\mid \theta)$
- But the likelihood $p(x\mid \theta)$ is **intractable / unavailable**
- Goal: learn the **posterior** $p(\theta \mid x)$anyway

### Why this is hard

- Classical Bayesian inference relies on evaluating $p(x\mid \theta)$
- ABC / SMC-ABC can work, but is often:
  - slow (many simulations)
  - sensitive to handcrafted summaries / distances
  - hard to scale to high dimensions

**Key need:** fast, accurate **likelihood-free** Bayesian inference that can be reused across many datasets.

---

## BayesFlow in one idea: amortize inference

<div class="mizzou-code-box">
BayesFlow is a framework for globally amortized Bayesian inference that uses invertible neural networks (normalizing flows) and learned summary statistics to perform likelihood-free inference efficiently and accurately across many datasets.
</div>

<br><br>

### What BayesFlow learns (high level)

- A **summary network** learns informative representations of the dataset $\tilde{x} = h_\psi(x_{1:N})$
- A **conditional normalizing flow** learns an invertible transform so we can:
  - **sample** quickly from $\theta \sim p(\theta \mid x)$
  - **evaluate** posterior densities via Jacobians (for calibration and diagnostics)
  
---  

## BayesFlow: summary + inference networks

::: columns
::: column
### Core idea

BayesFlow learns the inverse mapping
$$
x_{1:N} \;\mapsto\; p(\theta \mid x_{1:N})
$$
**once** using simulations, and reuses it for fast inference.

<br>

### Summary network
$$
\tilde{x} = h_\psi(x_{1:N})
$$
- Converts variable-length data to fixed summaries  
- Learns informative statistics automatically  
- Architecture matches data structure  
  
:::

::: column  
### Inference network (flow-based)

$$
\theta \;\xleftrightarrow[]{\text{invertible}}\; z,
\qquad z \sim \mathcal{N}(0,I)
$$

- Conditional invertible neural network (cINN)  
- Learns a bijective transport map
$$
\theta = f_\phi(z \mid \tilde{x})
$$
- Enables:
  - exact density evaluation (Jacobian)
  - exact posterior sampling (inverse pass)
:::
:::
  
--- 

## Notation and setup

::: columns
::: column
We consider a generative model with:

- Parameter vector  
  $$
  \theta = (\theta_1,\ldots,\theta_D) \in \mathbb{R}^D
  $$

- Dataset of size $N$  $x_{1:N} = (x_1,\ldots,x_N)$

<br>

### Neural components

- **Inference network (flow)** with parameters $\phi$
- **Summary network** with parameters $\psi$

::: 
::: column

<br><br><br>

### Latent space

We introduce a latent Gaussian variable
$$
z \sim \mathcal{N}(0, I),
$$
which will be mapped **invertibly** to $\theta$.

::: 
:::

## Learning the posterior via invertible maps

We seek an approximation
$$
p_\phi(\theta \mid x) \approx p(\theta \mid x)
$$
using an invertible neural network
$$
\theta = f_\phi(z; x),
\qquad
z \sim \mathcal{N}(0, I).
$$

<br>

### Change of variables

The induced posterior density is
$$
p_\phi(\theta \mid x)
=
p(z)\,
\left|
\det \frac{\partial f_\phi(z;x)}{\partial \theta}
\right|^{-1}.
$$



Thus, inference reduces to learning a **normalizing flow**
between $z$ and $\theta$, conditioned on data.

## Optimization objective

We minimize the expected KL divergence:
$$
\phi^\star
=
\arg\min_\phi
\mathbb{E}_{p(x)}
\big[
\mathrm{KL}(p(\theta\mid x)\,\|\,p_\phi(\theta\mid x))
\big].
$$



This is equivalent to maximizing:
$$
\mathbb{E}_{p(x,\theta)}
\big[
\log p_\phi(\theta \mid x)
\big].
$$



### Monte Carlo training

Using simulated pairs $(x^{(m)},\theta^{(m)})$:
$$
\mathcal{L}(\phi)
=
\frac{1}{M}\sum_{m=1}^M
\Big(
-\log p(z^{(m)})
- \log \big|\det J_{f_\phi}\big|
\Big).
$$

This objective is **fully tractable**.


## Variable-size datasets and summaries

When $N$ varies, raw data cannot be fed directly.

<br>

Introduce a learned summary:
$$
\tilde{x} = h_\psi(x_{1:N})
$$


The conditional flow becomes:
$$
\theta = f_\phi(z; \tilde{x}),
\qquad
\tilde{x} = h_\psi(x_{1:N}).
$$



### Joint objective

We now optimize:
$$
(\phi^\star,\psi^\star)
=
\arg\max_{\phi,\psi}
\mathbb{E}_{p(x,\theta)}
\big[
\log p_\phi(\theta \mid h_\psi(x_{1:N}))
\big].
$$

## Composing invertible networks

BayesFlow uses **chains of invertible transformations**
to increase expressiveness.


Let
$$
f_\phi = f_K \circ \cdots \circ f_1,
$$
so that
$$
\theta = f_K \circ \cdots \circ f_1(z).
$$



Each block $f_k$ is an **affine coupling block (ACB)**.



### Log-density

The log determinant decomposes:
$$
\log \left|\det J_{f_\phi}\right|
=
\sum_{k=1}^K
\log \left|\det J_{f_k}\right|.
$$

## Affine coupling blocks (ACB)

Split input into two parts:
$$
u = (u_1, u_2).
$$

Forward map:
$$
\begin{aligned}
v_1 &= u_1 \odot \exp(s_1(u_2)) + t_1(u_2), \\
v_2 &= u_2 \odot \exp(s_2(v_1)) + t_2(v_1).
\end{aligned}
$$



Inverse map exists in closed form:
$$
u = f^{-1}(v).
$$



### Key properties

- Determinant is cheap
- Internal networks need **not** be invertible



## Summary network: why do we need it?

In practice, the number of observations varies:
- different dataset sizes $N$
- different numbers of time points / measurements
- redundancy in raw observations

<br><br>

So instead of feeding raw $x_{1:N}$ directly into the cINN, we learn a fixed-size summary:
$$
\tilde{x} = h_\psi(x_{1:N}).
$$

<br><br>
<div class="mizzou-code-box">
**Goal:** avoid information loss from handcrafted summaries, and learn informative statistics directly from data.
</div>

## How $\tilde{x}$ enters the invertible network

<br>

Rather than feeding raw $x_{1:N}$ into each affine coupling block (ACB),
we condition the internal networks on the learned summary $\tilde{x}$.

<br><br>

For an ACB splitting $u=(u_1,u_2)$, the forward map becomes:
$$
v_1 = u_1 \odot \exp(s_1(u_2,\tilde{x})) + t_1(u_2,\tilde{x}),
$$
$$
v_2 = u_2 \odot \exp(s_2(v_1,\tilde{x})) + t_2(v_1,\tilde{x}).
$$

This keeps the overall map invertible, while injecting data information via $\tilde{x}$.

## Amortized Bayesian Inference With the BayesFlow Method {#AlgoBayesFlow}

```{ojs}

bayesflow_algo_anim575 = {
  const W = 1400, H = 520;

  const svg = d3.create("svg")
    .attr("viewBox", `0 0 ${W} ${H}`)
    .style("width","100%")
    .style("max-width", `${W}px`)
    .style("height", `${H}px`);

  // ---------- defs ----------
  const defs = svg.append("defs");

  defs.append("marker")
    .attr("id","arr")
    .attr("viewBox","0 0 10 10")
    .attr("refX", 9).attr("refY", 5)
    .attr("markerWidth", 8).attr("markerHeight", 8)
    .attr("orient","auto")
    .append("path")
    .attr("d","M 0 0 L 10 5 L 0 10 z")
    .attr("fill","rgba(0,0,0,0.55)");

  defs.append("filter").attr("id","sh")
    .append("feDropShadow")
      .attr("dx", 0).attr("dy", 1.2)
      .attr("stdDeviation", 1.2)
      .attr("flood-opacity", 0.18);

  const g = svg.append("g");

  // ---------- helpers ----------
  const box = (d, accent=false) => {
    const gg = g.append("g").attr("data-id", d.id);

    gg.append("rect")
      .attr("x", d.x).attr("y", d.y)
      .attr("width", d.w).attr("height", d.h)
      .attr("rx", 16)
      .attr("fill", "rgba(255,255,255,0.94)")
      .attr("stroke", accent ? "rgba(244,180,0,0.95)" : "rgba(0,0,0,0.16)")
      .attr("stroke-width", accent ? 3 : 1.6)
      .attr("filter","url(#sh)");

    gg.append("text")
      .attr("x", d.x + 16).attr("y", d.y + 28)
      .attr("font-size", 16).attr("font-weight", 850)
      .text(d.title);

    gg.append("text")
      .attr("x", d.x + 16).attr("y", d.y + 52)
      .attr("font-size", 12.5).attr("opacity", 0.82)
      .text(d.line1 || "");

    if (d.line2){
      gg.append("text")
        .attr("x", d.x + 16).attr("y", d.y + 72)
        .attr("font-size", 12.5).attr("opacity", 0.82)
        .text(d.line2);
    }
    return gg;
  };

  const center = d => [d.x + d.w/2, d.y + d.h/2];
  const midR   = d => [d.x + d.w,   d.y + d.h/2];
  const midL   = d => [d.x,         d.y + d.h/2];
  const midB   = d => [d.x + d.w/2, d.y + d.h];
  const midT   = d => [d.x + d.w/2, d.y];

  const curvedLR = (A,B) => {
    const [x1,y1] = A, [x2,y2] = B;
    const mx = (x1+x2)/2;
    return `M${x1},${y1} C${mx},${y1} ${mx},${y2} ${x2},${y2}`;
  };

  const curvedDown = (A,B, bow=60) => {
    const [x1,y1] = A, [x2,y2] = B;
    const my = (y1+y2)/2 + bow;
    return `M${x1},${y1} C${x1},${my} ${x2},${my} ${x2},${y2}`;
  };

  const loopBelow = (fromA, toB, yLaneBottom) => {
    const [x1,y1] = fromA;
    const [x2,y2] = toB;

    const yDown = yLaneBottom + 34;
    const pull = 140;

    return [
      `M${x1},${y1}`,
      `C${x1+pull},${y1} ${x1+pull},${yDown} ${x1},${yDown}`,
      `C${(x1+x2)/2},${yDown} ${(x1+x2)/2},${yDown} ${x2},${yDown}`,
      `C${x2-pull},${yDown} ${x2-pull},${y2} ${x2},${y2}`
    ].join(" ");
  };

  const loopBracket = ({x, y, w, h, label}) => {
    const gg = g.append("g");

    gg.append("rect")
      .attr("x", x).attr("y", y)
      .attr("width", w).attr("height", h)
      .attr("rx", 18)
      .attr("fill", "rgba(244,180,0,0.06)")
      .attr("stroke", "rgba(244,180,0,0.55)")
      .attr("stroke-width", 2)
      .attr("stroke-dasharray", "8,6");

    gg.append("text")
      .attr("x", x + 16).attr("y", y - 10)
      .attr("font-size", 13.5)
      .attr("font-weight", 850)
      .attr("opacity", 0.90)
      .text(label);

    return gg;
  };

  // ---------- layout ----------
  const yT = 120;
  const yI = 360;

  const train = [
    {id:"N",   x:70,   y:yT, w:210, h:92, title:"1) Sample N",   line1:"N ~ U(Nmin, Nmax)"},
    {id:"th",  x:320,  y:yT, w:210, h:92, title:"2) Sample Î¸",   line1:"Î¸^(m) ~ p(Î¸)"},
    {id:"sim", x:570,  y:yT, w:230, h:92, title:"3) Simulate x", line1:"x_i = g(Î¸^(m), Î¾_i)", line2:"for i = 1..N"},
    {id:"sum", x:840,  y:yT, w:210, h:92, title:"4) Summary",    line1:"xÌƒ = hÏˆ(xâ‚:N)"},
    {id:"fwd", x:1090, y:yT, w:210, h:92, title:"5) Flow (â†’)",   line1:"z = fÏ•(Î¸; xÌƒ)"},
  ];

  const infer = [
    {id:"obs", x:70,   y:yI, w:250, h:92, title:"Inference input", line1:"observed xâ‚:N^o"},
    {id:"sumo",x:360,  y:yI, w:210, h:92, title:"Summary",         line1:"xÌƒ^o = hÏˆ(xâ‚:N^o)"},
    {id:"z",   x:610,  y:yI, w:210, h:92, title:"Sample latent",   line1:"z(l) ~ N(0, I)"},
    {id:"inv", x:860,  y:yI, w:250, h:92, title:"Flow (â†)",        line1:"Î¸^(l) = fÏ•(z(l); xÌƒ^o)"},
    // tweak return to be explicit (optional)
    {id:"out", x:1150, y:yI, w:160, h:92, title:"Return",          line1:"{Î¸^(l)}_{l=1}^L"},
  ];

  const upd = {
    id:"updPanel", x:1040, y:235, w:260, h:96,
    title:"Training objective",
    line1:"compute loss ",
    line2:"update parameters: Ï•, Ïˆ"
  };

  // titles
  const tag = (x,y,text) => {
    g.append("text")
      .attr("x", x).attr("y", y)
      .attr("font-size", 14).attr("font-weight", 900)
      .attr("opacity", 0.85)
      .text(text);
  };
  tag(70, 95,  "TRAINING PHASE (repeat)");
  tag(70, 335, "INFERENCE PHASE (given observed data)");

  // entry/exit gates
  const gate = (x,y,label) => {
    const gg = g.append("g");
    gg.append("circle")
      .attr("cx",x).attr("cy",y).attr("r",10)
      .attr("fill","rgba(244,180,0,0.95)")
      .attr("stroke","rgba(0,0,0,0.18)");
    gg.append("text")
      .attr("x",x+16).attr("y",y+5)
      .attr("font-size", 13).attr("font-weight", 850)
      .attr("opacity", 0.85)
      .text(label);
  };
  gate(40, yT + 46, "enter repeat");
  gate(40, yI + 46, "exit with posterior samples");

  // draw boxes
  const nodes = [...train, ...infer, upd];
  const map = new Map(nodes.map(d => [d.id, d]));

  const nodeG = new Map();
  train.forEach(d => nodeG.set(d.id, box(d, false)));
  infer.forEach(d => nodeG.set(d.id, box(d, false)));
  nodeG.set(upd.id, box(upd, true));

  // loop annotations (m and i)
  const mLeft  = map.get("th").x - 18;
  const mTop   = map.get("th").y - 22;
  const mRight = map.get("fwd").x + map.get("fwd").w + 18;
  const mBot   = map.get("fwd").y + map.get("fwd").h + 18;

  loopBracket({
    x: mLeft,
    y: mTop,
    w: mRight - mLeft,
    h: mBot - mTop,
    label: "for m = 1..M  (mini-batch)"
  });

  const simBox = map.get("sim");
  loopBracket({
    x: simBox.x - 12,
    y: simBox.y - 14,
    w: simBox.w + 24,
    h: simBox.h + 28,
    label: "for i = 1..N  (simulate observations)"
  });

  // NEW: inference loop over l = 1..L (covers Sample latent + Flow inverse)
  const zBox = map.get("z");
  const invBox = map.get("inv");
  const lLeft  = zBox.x - 14;
  const lTop   = zBox.y - 22;
  const lRight = invBox.x + invBox.w + 14;
  const lBot   = invBox.y + invBox.h + 18;

  loopBracket({
    x: lLeft,
    y: lTop,
    w: lRight - lLeft,
    h: lBot - lTop,
    label: "for l = 1..L  (posterior samples)"
  });

  // main lane arrows
  const drawEdges = (arr) => {
    g.append("g")
      .selectAll("path.e")
      .data(arr)
      .join("path")
        .attr("fill","none")
        .attr("stroke","rgba(0,0,0,0.28)")
        .attr("stroke-width", 2.2)
        .attr("marker-end","url(#arr)")
        .attr("d", ([a,b]) => curvedLR(midR(map.get(a)), midL(map.get(b))));
  };

  drawEdges([["N","th"],["th","sim"],["sim","sum"],["sum","fwd"]]);
  drawEdges([["obs","sumo"],["sumo","z"],["z","inv"],["inv","out"]]);

  // Flow -> Training objective
  g.append("path")
    .attr("fill","none")
    .attr("stroke","rgba(0,0,0,0.28)")
    .attr("stroke-width", 2.2)
    .attr("marker-end","url(#arr)")
    .attr("d", curvedDown(midB(map.get("fwd")), midT(map.get("updPanel")), 70));

  // repeat loop: objective -> Sample N (under boxes)
  const yTrainBottom = yT + 92;
  g.append("path")
    .attr("fill","none")
    .attr("stroke","rgba(0,0,0,0.18)")
    .attr("stroke-width", 2.1)
    .attr("marker-end","url(#arr)")
    .attr("d", loopBelow(midL(map.get("updPanel")), midL(map.get("N")), yTrainBottom));

  // ---------- animation ----------
  const steps = [
    "N","th","sim","sum","fwd","updPanel","N",
    "obs","sumo","z","inv","out"
  ];

  const packet = g.append("circle")
    .attr("r", 6)
    .attr("fill","rgba(0,0,0,0.65)")
    .attr("opacity", 0.85);

  function setAccent(id){
    [...train, ...infer].forEach(d => {
      nodeG.get(d.id).select("rect")
        .attr("stroke","rgba(0,0,0,0.16)")
        .attr("stroke-width", 1.6);
    });
    nodeG.get("updPanel").select("rect")
      .attr("stroke","rgba(244,180,0,0.95)")
      .attr("stroke-width", 3);

    if (id !== "updPanel"){
      nodeG.get(id).select("rect")
        .attr("stroke","rgba(244,180,0,0.95)")
        .attr("stroke-width", 3);
    }
  }

  const lerp = (a,b,t) => a + (b-a)*t;

  function tick(tms){
    const T = 1200;
    const k = Math.floor((tms / T) % steps.length);
    const t = (tms / T) % 1;

    const a = steps[k];
    const b = steps[(k+1) % steps.length];

    setAccent(a);

    const A = center(map.get(a));
    const B = center(map.get(b));

    packet
      .attr("cx", lerp(A[0], B[0], t))
      .attr("cy", lerp(A[1], B[1], t));

    requestAnimationFrame(tick);
  }
  requestAnimationFrame(tick);

  return svg.node();
}


```

---




## Notation and setup

::: columns
::: column
### Normalizing Flows
Learn an invertible transformation between a simple base distribution and a complex target distribution.

$$
z = f_\theta(x), \qquad x = f_\theta^{-1}(z)
$$

Uses the change-of-variables formula:
$$
p(x) = p(z)\left|\det \frac{\partial f_\theta(x)}{\partial x}\right|
$$

Model complex probability densities by warping a simple distribution.

It learns a single invertible mapping $f_\theta$ with fixed parameters.


Inference is not amortized by default.
A new dataset typically requires retraining or re-optimization.

::: 
::: column

### Bayesian Flows (BayesFlow)
Learn an amortized inverse mapping from data to posterior distributions using simulations.

$$
x_{1:N} \;\longmapsto\; p(\theta \mid x_{1:N})
$$

1. Summary network $\tilde{x} = h_\psi(x_{1:N})$

2. Conditional invertible inference network
$$
\theta = f_\phi^{-1}(z; \tilde{x}), \qquad z \sim \mathcal{N}(0, I)
$$

Minimize the KL divergence between the true and approximate posterior using simulated data.

It leanrs A reusable inference network and a summary network aligned with probabilistic symmetries

Inference is amortized.
After training, posterior sampling is near-instant for new datasets.

::: 
:::


---

## Recent Developments in Normalizing Flows {#DevelopNormFlow}

Modern normalizing flows extend the basic idea of invertible density
transformations to achieve **scalability, expressiveness, and stability**
in high dimensions.


### Discrete-Time Flow Architectures

Most practical flows use **layered, discrete transformations**:
$$
z_K = f_K \circ f_{K-1} \circ \cdots \circ f_1(z_0),
\qquad z_0 \sim p_0(z)
$$

### Coupling-based flows

Split variables into two blocks:
$$
(x_A, x_B) \;\mapsto\; (y_A, y_B)
$$
with
$$
y_A = h(x_A; \Theta(x_B)), \qquad y_B = x_B.
$$

**Key property:**
- Triangular Jacobian
- $\log|\det J|$ computed cheaply

**Examples:** NICE, RealNVP, Glow

## Overview of Coupling flows

```{ojs}
nf_methods_timeline = {
  const W = 1000, H = 440;
  const svg = d3.create("svg")
    .attr("viewBox", `0 0 ${W} ${H}`)
    .style("width","100%")
    .style("max-width","980px")
    .style("height","440px");

  // ---------- defs ----------
  const defs = svg.append("defs");

  defs.append("marker")
    .attr("id","arrow")
    .attr("viewBox","0 0 10 10")
    .attr("refX", 9).attr("refY", 5)
    .attr("markerWidth", 8).attr("markerHeight", 8)
    .attr("orient","auto")
    .append("path")
    .attr("d","M 0 0 L 10 5 L 0 10 z")
    .attr("fill","rgba(0,0,0,0.55)");

  defs.append("filter").attr("id","glow")
    .append("feDropShadow")
      .attr("dx", 0).attr("dy", 0)
      .attr("stdDeviation", 2.2)
      .attr("flood-opacity", 0.30);

  // ---------- frame ----------
  svg.append("rect")
    .attr("x", 18).attr("y", 26)
    .attr("width", W-36).attr("height", H-52)
    .attr("rx", 10)
    .attr("fill","white")
    .attr("stroke","rgba(0,0,0,0.25)");

  svg.append("text")
    .attr("x", 22).attr("y", 18)
    .attr("font-size", 12)
    .attr("opacity", 0.8)
    .text("IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE");

  // ---------- left-side taxonomy ----------
  const items = [
    {sec:"Â§3.1", title:"Elementwise bijections", sub:"Non-linear elementwise transform",
     prob:"Problem: no mixing of variables", color:"#b00020"},
    {sec:"Â§3.2", title:"Linear flows", sub:"Affine combination of variables",
     prob:"Problem: limited representational power", color:"#b00020"},
    {sec:"Â§3.3", title:"Planar and radial flows", sub:"Non-linear transforms",
     prob:"Problem: hard to compute inverse", color:"#b00020"},
    {sec:"Â§3.4.1", title:"Coupling flows",
     sub:"Architectures that allow invertible non-linear transformations",
     prob:"Depend on coupling", color:"#1b7f3a"},
    {sec:"Â§3.4.2", title:"Autoregressive flows",
     sub:"Architectures that allow invertible non-linear transformations",
     prob:"Depend on coupling", color:"#1b7f3a"},
    {sec:"Â§3.5", title:"Residual flows", sub:"Invertible residual networks",
     prob:"", color:"#1b7f3a"},
    {sec:"Â§3.6", title:"Infinitesimal flows",
     sub:"Continuous flows depending on ODEs or SDEs",
     prob:"", color:"#1b7f3a"}
  ];

  const leftX = 42;
  const startY = 70;
  const rowH = 50;

  const rows = svg.append("g")
    .selectAll("g.row")
    .data(items)
    .join("g")
    .attr("transform", (d,i)=>`translate(${leftX},${startY+i*rowH})`);

  rows.append("rect")
    .attr("x",-12).attr("y",-18)
    .attr("width",460).attr("height",42)
    .attr("rx",10)
    .attr("fill","rgba(244,180,0,0.08)")
    .attr("stroke","rgba(244,180,0,0.85)")
    .attr("stroke-width",2.5)
    .attr("opacity",0)
    .attr("filter","url(#glow)");

  rows.append("text")
    .attr("x",0).attr("y",-2)
    .attr("font-size",16)
    .attr("font-weight",700)
    .text(d=>`${d.sec}  ${d.title}`);

  rows.append("text")
    .attr("x",20).attr("y",16)
    .attr("font-size",12.5)
    .attr("fill","#1b7f3a")
    .text(d=>d.sub);

  rows.append("line")
    .attr("x1",330).attr("x2",380)
    .attr("y1",-6).attr("y2",-6)
    .attr("stroke","rgba(0,0,0,0.5)")
    .attr("stroke-width",1.8);

  rows.append("path")
    .attr("d","M380,-6 L450,-6")
    .attr("stroke","rgba(0,0,0,0.5)")
    .attr("stroke-width",1.8)
    .attr("marker-end","url(#arrow)");

  rows.append("text")
    .attr("x",462).attr("y",-2)
    .attr("font-size",12.5)
    .attr("font-weight",650)
    .attr("fill",d=>d.color)
    .text(d=>d.prob);

  // ---------- coupling-functions box (MOVED DOWN) ----------
  const cf = {
    x: 640,
    y: 210,          // << MOVED DOWN
    w: 300,
    h: 210,
    bullets: [
      "Affine",
      "Non-linear squared",
      "Continuous mixture CDFs",
      "Splines",
      "Neural autoregressive",
      "Sum-of-squares polynomial",
      "Piecewise-bijective"
    ]
  };

  svg.append("rect")
    .attr("x",cf.x).attr("y",cf.y)
    .attr("width",cf.w).attr("height",cf.h)
    .attr("rx",10)
    .attr("fill","white")
    .attr("stroke","rgba(0,0,0,0.45)")
    .attr("stroke-width",1.6);

  svg.append("text")
    .attr("x",cf.x+14).attr("y",cf.y+26)
    .attr("font-size",15)
    .attr("font-weight",750)
    .text("Â§3.4.4  Coupling functions");

  svg.selectAll("text.cf")
    .data(cf.bullets)
    .join("text")
      .attr("x",cf.x+18)
      .attr("y",(d,i)=>cf.y+54+i*22)
      .attr("font-size",13)
      .text(d=>"â€¢ "+d);

  // ---------- connectors ----------
  function connector(y0){
    const x1 = leftX + 380;
    const x2 = cf.x - 12;
    const y2 = cf.y + 22;
    const mx = (x1+x2)/2 + 40;
    return `M${x1},${y0} C${mx},${y0} ${mx},${y2} ${x2},${y2}`;
  }

  const conn = svg.append("path")
    .attr("fill","none")
    .attr("stroke","rgba(0,0,0,0.35)")
    .attr("stroke-width",2)
    .attr("marker-end","url(#arrow)");

  // ---------- animation ----------
  let k = 0;
  const T = 1700;

  function focus(i){
    rows.select("rect").attr("opacity",0);
    rows.filter((d,idx)=>idx===i)
      .select("rect").attr("opacity",1);

    if (i===3 || i===4){
      const y0 = startY + i*rowH - 6;
      conn.attr("d",connector(y0)).attr("opacity",1);
    } else {
      conn.attr("opacity",0);
    }
  }

  function loop(){
    focus(k);
    k = (k+1)%items.length;
    d3.timeout(loop,T);
  }

  focus(0);
  d3.timeout(loop,T);

  return svg.node();
}
```



## Autoregressive and Spline Flows

### Autoregressive flows

Each dimension depends on previous ones:
$$
y_t = h\big(x_t;\,\Theta_t(x_{1:t-1})\big)
$$

- **MAF**: fast density evaluation  
- **IAF**: fast sampling  

<br> <br><br>

### Neural spline flows

Replace affine maps with **monotone splines**:
- Rational quadratic splines
- Piecewise polynomial transforms

**Key insight:**
> Nonlinear monotone transformations dramatically increase expressiveness
without sacrificing invertibility.

---

## Continuous-Time Normalizing Flows

Flows can be defined as **continuous dynamics**:
$$
\frac{dx(t)}{dt} = F(x(t), t)
$$

Density evolves via:
$$
\frac{d}{dt} \log p(x(t))
=
-\mathrm{Tr}\!\left(\frac{\partial F}{\partial x}\right)
$$

**Representative methods:**
- Neural ODE flows
- FFJORD (stochastic trace estimator)

**Advantages:**
- No explicit Jacobian determinant
- Parameter-efficient
- Smooth transformations

---

## Summary of Normalizing Flow Families (I) {#SummNormFlow}

| Flow family | Core transformation | Key idea | Strengths | Limitations |
|------------|--------------------|----------|-----------|-------------|
| Elementwise bijections | $$z_i = f_i(x_i)$$ | Independent nonlinear transforms | Simple, cheap Jacobian | No variable mixing |
| Linear flows | $$z = Ax + b$$ | Global affine transform | Mixes variables, exact inverse | Limited expressiveness |
| Planar flows | $$z = x + u\,h(w^\top x + b)$$ | Rank-1 nonlinear deformation | Flexible, fast Jacobian | Inverse often intractable |
| Radial flows | $$z = x + \beta\,h(\alpha, r)(x - x_0)$$ | Radial contraction/expansion | Can induce multimodality | Hard inverse |
| Coupling flows | $$z_{1:d}=x_{1:d}$$ $$z_{d+1:D}=x_{d+1:D}\odot e^{s(x_{1:d})}+t(x_{1:d})$$ | Split variables, conditional transforms | Exact inverse, scalable | Needs good coupling design |


## Summary of Normalizing Flow Families (II)
<br><br>

| Flow family | Core transformation | Key idea | Strengths | Limitations |
|------------|--------------------|----------|-----------|-------------|
| Autoregressive flows | $$z_i = f_i(x_i; x_{<i})$$ | Sequential conditional transforms | Highly expressive | Slow sampling or likelihood |
| Residual flows | $$z = x + f(x)$$ | Invertible residual networks | Deep & flexible | Requires Lipschitz control |
| Continuous (Neural ODE) flows | $$\frac{dz(t)}{dt} = f(z(t), t)$$ | Continuous-time transformation | Memory efficient, smooth | Expensive ODE solves |

---

## Future Directions of Normalizing Flows {#FutureDir}

**Next-Generation Generative Modeling & Inference**

-   **Continuous-Time Flows:** Moving beyond discrete layers to Neural ODEs for more flexible, invertible transformations.
-  **Geometric & Graph Flows:** Adapting flows to non-Euclidean data (graphs, manifolds) for structural biology and chemistry.
-   **Efficiency & Scalability:** Reducing computational overhead for high-dimensional data (e.g., high-res images, 3D modeling).
-   **Hybrid Architectures:** Combining flows with diffusion models or VAEs to leverage exact likelihoods and high-fidelity generation.
-   **Beyond Gaussian Base:** Exploring more expressive, non-Gaussian, or learnable base distributions.

<br>

### Key Research Areas
1.  Riemannian Manifold Flows
2.  Adaptive Step Size ODEs
3.  Implicit Regularization Techniques


##  {data-background-image="assets/tigerstripes.png" data-background-size="cover" data-background-position="center"}

<div class="thankyou-slide">

<div class="thankyou-content">

Thank you for your attention!


</div>

</div>


